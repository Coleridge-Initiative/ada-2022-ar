{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><br>\n",
    "    Arkansas Work-Based Learning to Workforce Outcomes <br>\n",
    "    Applied Data Analytics Training | Spring 2022\n",
    "    <h1> Characterizing Demand: Unsupervised Machine Learning </h1>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Coleridge Initiative</a>\n",
    "    </span>\n",
    "    <center> Joshua Edelmann, Rukhshan Arif Mian, Benjamin Feder</center>\n",
    "</center>\n",
    "\n",
    "<a href=\"https://doi.org/10.5281/zenodo.10407957\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.10407957.svg\" alt=\"DOI\"></a>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we do not have a clear outcome variable but nevertheless want to explore and discover any inherent groupings or configurations in the data. Unsupervised machine learning methods can help tackle these problems. Clustering is the most common unsupervised machine learning technique, but you might also be aware of principal components analysis (PCA) or neural networks implementations such as self-organizing maps (SOM). This notebook will provide an introduction to unsupervised machine learning through a clustering example.\n",
    "\n",
    "In this example, we will try to identify patterns within Arkansas' labor market – namely patterns in the types of employers. Can we isolate specific employers based on derived features addressing their employees' experiences as measured by stability, opportunity, quality, and firm characteristics? We hope to do so using an unsupervised machine learning algorithm. Once we cluster Arkansas' employers, we can explore how apprenticeship completers in 2015 fit within the scope of Arkansas' labor market based on its 2014 characterization, as this would be the most data recent labor market data available to 2015 completers, in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will focus on **K-Means clustering** (*k* defines the number of clusters), which is considered to be the most commonly-used clustering method. The algorithm works as follows:\n",
    "1. Select *k* (the number of clusters you want to generate).\n",
    "2. Initialize by selecting k points as centroids of the *k* clusters. This is typically done by selecting k points uniformly at random.\n",
    "3. Assign each point a cluster according to the nearest centroid.\n",
    "4. Recalculate cluster centroids based on the assignment in **(3)** as the mean of all data points belonging to that cluster.\n",
    "5. Repeat **(3)** and **(4)** until convergence. Convergence will be further discussed in the sections below.\n",
    "\n",
    "Please reference Chapter 7 of the Big Data and Social Science Textbook and the accompanying class videos for more information on unsupervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how *k*-means clustering can be used to better understand Arkansas' labor market in 2014. We've already developed a handful of employer measures in a supplemental notebook (`Supplemental_Employer_Measures.ipynb`). We will experiment with a few different values of *k* to see how we can best understand the labor market by looking for differentiation between each of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Set Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main R package that we will use for clustering is called `cluster`. We also import all our usual packages for database connection and data manipulation/visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "# Database interaction imports\n",
    "suppressMessages(library(odbc))\n",
    "\n",
    "# For data manipulation/visualization\n",
    "suppressMessages(library(tidyverse))\n",
    "\n",
    "# For faster date conversions\n",
    "suppressMessages(library(lubridate))\n",
    "\n",
    "# Use percent() function\n",
    "suppressMessages(library(scales))\n",
    "\n",
    "suppressMessages(library(zoo))\n",
    "\n",
    "# clustering\n",
    "suppressMessages(library(cluster))\n",
    "options(warn=0)\n",
    "\n",
    "# set seed to ensure work is reproducible because k-means has random starting points-seed for the elbow plot\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server\n",
    "con <- DBI::dbConnect(odbc::odbc(),\n",
    "                     Driver = \"SQL Server\",\n",
    "                     Server = \"msssql01.c7bdq4o2yhxo.us-gov-west-1.rds.amazonaws.com\",\n",
    "                     Trusted_Connection = \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adjusting overall graph attributes\n",
    "\n",
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 12, repr.plot.height = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "Now that we have set up the environment, we can start getting ready to deploy the clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read in a table **employer_yearly_agg**, which contains characteristics of Arkansas' labor market from 2011-2021, and limit it to information to 2014. The table contains variables tracking the following characteristics on a quarterly basis, with the average (`avg_`) quarterly values reported within each year. The measures can be broken up within three separate categories, which are covered in the `04_Characterizing_Demand_Beginner.ipynb` notebook.  For more information as to how this table was created, please refer to the Supplemental notebook [\"Supplemental_Employer_Measures.ipynb.\"](Supplemental/Supplemental_Employer_Measures.ipynb).\n",
    "\n",
    "> Note: It is possible to cluster on all employers in all years at the same time as well—just keep in mind that there are a different set of assumptions that are prevalent with that decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read aggregated employer data\n",
    "qry <- \"\n",
    "select *\n",
    "from tr_ar_2022.dbo.employer_yearly_agg\n",
    "where year = 2014\n",
    "\"\n",
    "\n",
    "emp <- dbGetQuery(con, qry)\n",
    "\n",
    "head(emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ratio_full_total measure in R\n",
    "emp <- emp %>% \n",
    "    mutate(ratio_full_total = avg_full_num_employed/avg_num_employed)\n",
    "\n",
    "head(emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will sort the data in ascending order (based on all columns). SQL may read in the data in a different order every time we re-run our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do.call constructs and executes a function call from a name or a function and a list of arguments to be passed to it.\n",
    "# we call the order function on all of emp's columns\n",
    "emp <- emp[do.call(order, emp), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the Data\n",
    "\n",
    "A dataset must contain specific attributes in order to run a k-means clustering algorithm. We will preprocess `emp` through three steps:\n",
    "\n",
    "1. Remove non-explanatory and non-continuous features\n",
    "2. Examine scales across variables\n",
    "3. Analyze missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-Explanatory and Non-Continuous Features\n",
    "\n",
    "For any algorithm, non-explanatory variables such as unique ids should be removed from the data set. Additionally, k-means algorithms only work properly with continuous features. This is partly due to the distance measures that k-means algorithms use. The one we will be using is the euclidean distance, which calculates the distance between each data point and the centroid of every cluster. It is difficult to assign positions for categorical variables in the euclidean space.\n",
    "\n",
    "In this case, we we will remove the following columns (features):\n",
    "\n",
    "- **federal_ein**: Employer Number\n",
    "- **year**: Year (always 2014)\n",
    "- **NAICS_National_Industry_ID**: Employer's naics code\n",
    "- **two_digit_naics**: Employer's 2-digit naics code\n",
    "\n",
    "> Note: There are more sophisticated clustering algorithms that do not use Euclidean distances and thus allow categorical variables in the model. If you are interested in them, you can take a look at the functions `kmodes` and `gower.dist` in R - you will need to load their respective libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features without explanatory power\n",
    "emp_ml <- emp %>%\n",
    "    select(-c(federal_ein, year, NAICS_National_Industry_ID, two_digit_naics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Scales Across Variables\n",
    "\n",
    "Next, we use the `str` function to see if there are non-numeric variables that remain in our data frame (now called `emp_ml`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data type of all variables - make sure all of them are numeric\n",
    "str(emp_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the different explanatory metrics are on a variety of numerical scales, the k-means algorithm will overweigh variables on larger scales due to its distance metric. For this purpose, we first convert all variables to a **numeric** type and scale them.\n",
    "\n",
    "Note that we do see variables with the type **int** and **integer64** – we are going to convert these to a **numeric** (or **num**) type as well, as integers do not scale in R. \n",
    "\n",
    "We utilize the `sapply()` function to do this conversion. It takes a data frame (or a list/vector) as input and gives output in a vector or matrix. It allows us to batch convert all columns in our data frame to a numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all numeric variables to numeric type otherwise integer64 won't scale using sapply\n",
    "emp_ml_num <- emp_ml %>%\n",
    "    sapply(as.numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can use the `scale()` function to scale all columns in our data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features since variables like avg_emp_rate have a much smaller range than avg_total_earnings\n",
    "emp_ml_scale <- scale(emp_ml_num)\n",
    "\n",
    "# View first rows after scaling\n",
    "emp_ml_scale %>% \n",
    "   head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Missingness\n",
    "\n",
    "If an employer has missing information in any of the columns, the row will be dropped in the clustering method.\n",
    "\n",
    "> Note: You should **never remove data** if possible - in a real world setting you would likely want to fill any missing data with an imputation method or a baseline assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows (where each row is a unique employer/year combination)\n",
    "nrow(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na.omit will remove any rows with any NA values\n",
    "emp_ml_scale <- na.omit(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows after dropping rows with any NA values\n",
    "# see that there is no missing data\n",
    "nrow(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using `na_omit()` to remove missing values in our data frame of interest (**emp_ml_scale**), we see that our number of observations does not change – this means that the data frame does not contain missing values in any of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose the Number of Clusters, *K*\n",
    "\n",
    "Running a *k*-means model is simple: we just need to use `kmeans()` and choose the number of clusters (called `centers`). What number should we choose? Here, we have a bunch of features, so it can be difficult to visualize the data in order to choose the proper number.\n",
    "\n",
    "Luckily, there are a few procedures we can use to help us find an appropriate *k* value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technique that can be used to help determine *k* in the elbow method. In the elbow method, we try different k values and calculate the sum of squared errors (`SSE`) after the model converges. Then we plot all the `SSE` by K in a line-chart. The line-chart should resemble an arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we define a function called `wss()`. It allows us to compute the total within-cluster sum of squares. Functions are useful when you want to perform a certain task multiple times. A function accepts input arguments and produces the output by executing valid R commands that are inside the function. \n",
    "\n",
    "Within this function, we will use `nstart = 20`. In other scenarios, you may see `nstart = 1` to minimize the code runtime, as `nstart` specifies a number of initial configurations and reports on the best one. Due to the size of the data, if you increase `nstart` in the function above, some iterations may not converge based on the number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: An optimal number for `nstart` is usually somewhere between 20 and 50. (See more information in the References section - Rebecca Steorts, Duke University)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute total within-cluster sum of squares\n",
    "# we can run this for multiple values of k – showcased later in this notebook\n",
    "wss <- function(k) {\n",
    "    kmeans(emp_ml_scale, centers=k, nstart=20)$tot.withinss\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define `k.values` that takes values between 1 and 15 (inclusive). We utilize `map_dbl()` to run the `wss()` function 15 times. That is, we run `wss` for `k=1`, `k=2` up until `k=15`. \n",
    "\n",
    "`map_dbl()` allows us to do this in one line of code. Resulting values for each run are stored in **wss_values**. This code chunk will take about 2 minutes to finish running, and you may see a repeated warning message, particularly around convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot wss for k =1 to k = 15\n",
    "k.values <- 1:15\n",
    "\n",
    "# extract wss values for each k\n",
    "wss_values <- map_dbl(k.values, wss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `wss_values`, we can plot these using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wss_df <- data.frame(wss_values, k.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting wss_df\n",
    "wss_df %>%\n",
    "    ggplot(aes(x=k.values, y=wss_values)) + \n",
    "    geom_line() + \n",
    "    geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to choose the number around the inflection point, where the change in the sum of squared errors becomes negligible, indicating that there is little room to improve the model by increasing k (the bend in the elbow). On our graph, the elbow curve begins to flatten around k = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Model\n",
    "\n",
    "Now that we have chosen a value for **k** using the elbow method, we move forward and utilize this using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and run on emp_ml_scale with centers = 4\n",
    "# need to set a different seed to ensure static results for a separate random assignment\n",
    "set.seed(2)\n",
    "k4 <- kmeans(emp_ml_scale, centers = 4, nstart = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `kmeans` function returns the following components, with the most useful for us now as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(k4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where: \n",
    "\n",
    "- `cluster` - an integer indicating a cluster to which each point is allocated\n",
    "- `centers` - a matrix of cluster centers\n",
    "- `totss` - the total sum of squares\n",
    "- `withinss` - vector of within-cluster sum of squares, one component per cluster.\n",
    "- `tot.withinss` - total within-cluster sum of squares, i.e. `sum(withinss)`\n",
    "- `betweenss` - the between-cluster sum of squares, i.e. `totss-tot.withinss`\n",
    "- `size` - the number of points in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the size of each cluster by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see size of cluster\n",
    "k4$size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the employers are concentrated two of the clusters. In the perfect world, we would want them to be distributed more evenly across clusters, but in some cases, it may make sense that they wouldn't. \n",
    "\n",
    "We will continue to run our model through a battery of tests to further inform our clustering decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Features across Clusters\n",
    "\n",
    "To start to tease out the potential for high intra-cluster and low inter-cluster similarity, we can take a look at basic descriptives of the employers in these clusters. This will allow us to start to get a sense of some of the important differences across the clusters in the hopes of categorizing each cluster as a separate \"type\" of employer relative to those in other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to add the clusters to the **emp** data frame so we know which employers fall into which cluster. R preserves the ordering when creating the clusters so we can essentially stick the clusters to the end of the data frame without needing a crosswalk file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster number to the original dataframe\n",
    "frame_4 <- emp %>% \n",
    "    mutate(k4.cluster = k4$cluster)\n",
    "\n",
    "\n",
    "head(frame_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a separate data frame without the non-continuous variables (besides **k4.cluster**) so that we can easily summarize the features across the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove federal_ein, year, and naics codes related columns\n",
    "frame_4_few_cols <- frame_4 %>%\n",
    "    select(-c(federal_ein, year, NAICS_National_Industry_ID, two_digit_naics))\n",
    "\n",
    "# summarize and add in sizes of each cluster\n",
    "frame_4_few_cols %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    # getting averages for each cluster\n",
    "    # add suffix \"by_employer\" to each summarize variable\n",
    "    summarise(across(everything(), # adds the suffix across every column in our dataframe\n",
    "                     list(by_employer=mean))) %>%\n",
    "    mutate(\n",
    "        size = k4$size\n",
    "    ) %>%\n",
    "    # relocates the size column after the k4.cluster columns\n",
    "    relocate(size, .after=k4.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can see that our biggest cluster, cluster 3, contains relatively REDACTED sized employers with REDACTED stability, REDACTED opportunity, and REDACTED firm characteristics. In contrast, cluster 4 contains some of the REDACTED employers in the state with REDACTED firm characteristics.\n",
    "\n",
    "> Note: When clustering, be cognizant that small numbers of employers per cluster may result in additional redaction due to disclosure regulations concerning the number of employers contributing to each cell. More on this in the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Key Variables of Interest\n",
    "\n",
    "We can also visualize the differences between in clusters in more detail by including the standard deviation in conjunction with the average for some of the differentiating features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on avg_avg_earnings, avg_bottom_25_pctile, avg_top_75_pctile (all measures of job quality)\n",
    "# need to convert summarize data frame into a long format with each variable/cluster combination corresponding to a single row\n",
    "frame_4_mean <- frame_4_few_cols %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    select(k4.cluster, avg_avg_earnings, avg_bottom_25_pctile, avg_top_75_pctile) %>%\n",
    "    summarise(across(everything(), mean)) %>%\n",
    "    # reshaping the date frame from wide to long\n",
    "    # pivot_longer \"lengthens\" data, increasing the number of rows and decreasing the number of columns (from: https://tidyr.tidyverse.org/)\n",
    "    pivot_longer(-k4.cluster, names_to = \"variable\", values_to = \"mean\")\n",
    "\n",
    "# Save results with standard deviation to a dataframe\n",
    "frame_4_sd <- frame_4_few_cols %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    select(k4.cluster, avg_avg_earnings, avg_bottom_25_pctile, avg_top_75_pctile) %>%\n",
    "    summarise(across(everything(), sd)) %>%\n",
    "     # reshaping the date frame from wide to long\n",
    "    # pivot_longer \"lengthens\" data, increasing the number of rows and decreasing the number of columns (from: https://tidyr.tidyverse.org/)\n",
    "    pivot_longer(-k4.cluster, names_to = \"variable\", values_to = \"sd\") %>%\n",
    "    select(-c(k4.cluster, variable))\n",
    "\n",
    "# Bind two dataframes together\n",
    "df <- cbind(frame_4_mean,frame_4_sd)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare mean + sd across all four clusters for variables of interest\n",
    "df %>%\n",
    "ggplot(aes(x=as.character(k4.cluster),\n",
    "           y=mean, \n",
    "           fill=as.character(k4.cluster))) +\n",
    "    geom_bar(stat=\"identity\", position = position_dodge()) +   # plot bars for the mean values\n",
    "    geom_errorbar(aes(ymax= mean + sd, ymin = mean),            # add standard deviation bars\n",
    "                  width=.2,\n",
    "                  position = position_dodge(.9)) +\n",
    "    facet_grid(. ~ variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can really see the differences in firm characteristics across the clusters as well as where there may be potential overlap. There are additional steps you can take to further justify a clustering choice, but in clustering there may not be a single right answer - every time we run a different number of clusters, interesting patterns about our data may emerge. It may be useful to try running the algorithm on different numbers of clusters and comparing the outputs between models.\n",
    "\n",
    "We really want to know *whether the clusters that we find represent true subgroups in our data*. This could be a crucial input toward choosing the right number of clusters. (See more information on additional methods for selecting `k` in the References section - Rebecca Steorts, Duke University).\n",
    "\n",
    "One way we can start to do so is by analzing the most common industries of the employers in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common naics by cluster\n",
    "frame_4 %>%\n",
    "    group_by(k4.cluster, two_digit_naics) %>%\n",
    "    summarize(count_most_freq = n_distinct(federal_ein)) %>%\n",
    "    arrange(k4.cluster, desc(count_most_freq)) %>%\n",
    "    ungroup() %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    mutate(total = sum(count_most_freq)) %>%\n",
    "    filter(row_number()==1) %>%\n",
    "    mutate(perc = round(count_most_freq*100/total, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relating Back to the Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with descriptions of employers existing within Arkansas' labor force, we can now relate the results of our unsupervised machine learning model back to our cohort of interest.  In this section we will take a look at how a subset of our original cohort fit within these clusters. The subset we are interested in is 2015 completers, and their earnings outcomes for their primary employer in the following four quarters. In your own analysis you may be interested in the entire cohort or a subset as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dimensional model to get primary employer information\n",
    "qry <- \"\n",
    "SELECT\n",
    "F.Quarter_ID - P.Apprenticeship_End_Quarter_ID AS Quarters_Relative_to_Completion,\n",
    "P.Person_ID,\n",
    "F.Primary_Employer_Wages ,\n",
    "PE.Federal_EIN\n",
    "FROM \n",
    "tr_ar_2022.dbo.nb_cohort C --COHORT\n",
    "JOIN tr_ar_2022.dbo.AR_MDIM_Person P ON (P.Apprentice_Number=C.apprnumber) --PERSON\n",
    "JOIN tr_ar_2022.dbo.AR_FACT_Quarterly_Observation F --QUARTERLY OBSERVATION FACT\n",
    "\tON (P.Person_ID=F.Person_ID) \n",
    "\tAND (F.Quarter_ID BETWEEN (P.Apprenticeship_End_Quarter_ID + 1) AND (P.Apprenticeship_End_Quarter_ID+4))  --4 QTRS POST COMPLETION\n",
    "JOIN tr_ar_2022.dbo.AR_RDIM_NAICS_National_Industry NNI ON (P.Apprenticeship_NAICS_National_Industry_ID=NNI.NAICS_National_Industry_ID) --APPRENTICESHIP INDUSTRY\n",
    "JOIN tr_ar_2022.dbo.AR_MDIM_Employer PE ON (PE.Employer_ID=F.Primary_Employer_ID)  --PRIMARY EMPLOYER\n",
    "WHERE P.Apprenticeship_Completer='Y' and YEAR(C.exitwagedt) = 2015\n",
    "\"\n",
    "\n",
    "df_wages <- dbGetQuery(con, qry)\n",
    "\n",
    "head(df_wages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Join wages table with frame_4 clustering results\n",
    "df_wages_clus <- df_wages %>%\n",
    "    inner_join(frame_4, by=c('Federal_EIN' = 'federal_ein'))\n",
    "\n",
    "df_wages_clus %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employers who Primarily Employed a Member of the Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the number of employers by cluster that employed someone in the cohort. Keep in mind a cohort of 2015 apprenticeship completers already limits the size of the data frame substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see number of employers by cluster that primarily employed someone in the cohort\n",
    "df_wages_clus %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_cohort = n_distinct(Federal_EIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see number of employers by cluster that primarily employed someone in the cohort by quarter\n",
    "df_wages_clus %>%\n",
    "    group_by(k4.cluster, Quarters_Relative_to_Completion) %>%\n",
    "    summarise(emp_cohort = n_distinct(Federal_EIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare within cohort employers to all employers in original clusters\n",
    "# Get number of unique employers per cluster in the full dataframe (all employers)\n",
    "frame_4 %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_all = n_distinct(federal_ein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with percentages\n",
    "cohort_emp <- df_wages_clus %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_cohort = n_distinct(Federal_EIN))\n",
    "\n",
    "emp_all <- frame_4 %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_all = n_distinct(federal_ein))\n",
    "\n",
    "# Join cohort employers with all employers, andd find percentage\n",
    "cohort_emp %>%\n",
    "    inner_join(emp_all, by = 'k4.cluster') %>%\n",
    "    mutate(percentage = (emp_cohort / emp_all) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, one limitation of our original employers file is that it doesn't include quarterly information for employers with fewer than 5 employees in a specific quarter. We can see that the majority of employers in these clusters REDACTED primarily employ anyone in our cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Wages over time\n",
    "\n",
    "Next, we look at the average wages for employees primarily employed by employers within each cluster over time. To do this, we first calculate the mean earnings for our cohort and the number of individuals for each cluster-quarter combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(df_wages_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average quarterly earnings and number individuals employers in at least one quarter for cohort by cluster\n",
    "df_avg_wages_clus <- df_wages_clus %>%\n",
    "    group_by(k4.cluster, Quarters_Relative_to_Completion) %>%\n",
    "    summarise(\n",
    "        mean_earnings_cohort = mean(Primary_Employer_Wages),\n",
    "        num_individuals = n_distinct(Person_ID)\n",
    "    )\n",
    "\n",
    "df_avg_wages_clus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot the average wages by cluster and quarter to understand outcomes for our cohort over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_wages_clus %>%\n",
    "    ggplot(aes(x = Quarters_Relative_to_Completion, y = mean_earnings_cohort, color=as.character(k4.cluster)))  +  \n",
    "    geom_line() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that among our cohort, those who primarily worked for employers in clusters 1 and 4 earned more, on average while those employed in cluster 3 earned the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_wages_clus %>% \n",
    "    group_by(k4.cluster) %>% \n",
    "    summarize(mean = mean(mean_earnings_cohort)) %>% \n",
    "    arrange(desc(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Rebecca Steorts, Assistant Professor, Duke University, Department of Statistical Science, Data Mining and Machine Learning course: https://github.com/resteorts/data-mine/tree/master/lectures_2018/10-unsupervise/10-kmeans.pdf\n",
    "\n",
    "TDC Characterizing Demand, Advanced (more to come)\n",
    "\n",
    "UC Business Analytics R Programming Guide: https://uc-r.github.io/kmeans_clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
